{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyWavelet modules\n",
    "from pyWavelet import snowradar, picklayers, matfunc\n",
    "\n",
    "#Python stdlib\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "#Community packages\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore') # suppress divide-by-zero warnings in below plots\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_snow = 0.300 #Density of snow on sea ice; See Alexandrov et al., 2010, The Cryosphere\n",
    "perm_snow = (1 + 0.51 * density_snow) ** 3  #Density to permitivity\n",
    "n_snow = np.sqrt(perm_snow) #Permitivity to refractive index\n",
    "c = 299792458 #Vacuum speed of light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#radar_file = \"./data/eureka/sr/Data_20160419_04_025.mat\"\n",
    "\n",
    "radar_file = \"E:/OIB/snowradar/CSARP_deconv/20170310_01/Data_20170310_01_088.mat\"\n",
    "#radar_file = \"./data/eureka/sr/Data_20160419_04_025.mat\"\n",
    "\n",
    "#radar_dat = snowradar.OIB(radar_file, l_case='full') #Use AWI child class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "try:\n",
    "    radar_dat = matfunc.loadmat(radar_file)\n",
    "except NotImplementedError:\n",
    "    radar_dat =  h5py.File(radar_file, 'r')\n",
    "    radar_dat = matfunc.h5todict(radar_dat, exclude_names='#refs#')\n",
    "except:\n",
    "    ValueError('Could not read SnowRadar file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 1104)\n",
      "(1036, 1104)\n"
     ]
    }
   ],
   "source": [
    "print(radar_dat['Data'].shape)\n",
    "print(np.squeeze(radar_dat['Data']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oib_2016 = \"./data/eureka/sr/Data_20160419_04_025.mat\"\n",
    "oib_2017 = \"E:/OIB/snowradar/CSARP_deconv/20170310_01/Data_20170310_01_088.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyWavelet import snowradar, picklayers, matfunc\n",
    "import h5py\n",
    "\n",
    "\n",
    "try:\n",
    "    import scipy.io as sio\n",
    "    radar_dat = matfunc.loadmat(oib_2017)\n",
    "\n",
    "except NotImplementedError:\n",
    "    radar_dat =  h5py.File(oib_2017, 'r')\n",
    "    radar_dat = matfunc.h5todict(radar_dat, exclude_names='#refs#')\n",
    "\n",
    "except:\n",
    "    ValueError('Could not read at all...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([482.88414085])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radar_dat['Elevation'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#refs#\n",
      "Data\n",
      "Elevation\n",
      "Elevation_Correction\n",
      "GPS_time\n",
      "Heading\n",
      "Latitude\n",
      "Longitude\n",
      "Pitch\n",
      "Roll\n",
      "Surface\n",
      "Time\n",
      "Truncate_Bins\n",
      "Truncate_Mean\n",
      "Truncate_Median\n",
      "Truncate_Std_Dev\n",
      "custom\n",
      "file_version\n",
      "param_qlook\n",
      "param_records\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    radar_dat_2016 = matfunc.loadmat(oib_2016)\n",
    "except NotImplementedError:\n",
    "    radar_dat_2016 = h5py.File(oib_2016, 'r')[\"/\"]\n",
    "except:\n",
    "    ValueError('Could not read at all...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    radar_dat_2017 = matfunc.loadmat(oib_2017)\n",
    "except NotImplementedError:\n",
    "    radar_dat_2017 = h5py.File(oib_2017, 'r')[\"/\"]\n",
    "except:\n",
    "    ValueError('Could not read at all...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/\" (20 members)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radar_dat_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(radar_dat_2017['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_names = None\n",
    "if (exclude_names is not None):\n",
    "    print('okl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = h5py.File(oib_2017, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5todict(h5f, path=\"/\", exclude_names=None):\n",
    "    ddict = {}\n",
    "    for key in h5f[path]:\n",
    "        if exclude_names is not None:\n",
    "            if key in exclude_names:\n",
    "                continue\n",
    "        if isinstance(h5f[path + \"/\" + key], h5py._hl.group.Group):\n",
    "            ddict[key] = h5todict(h5f, path + \"/\" + key)\n",
    "        else:\n",
    "            ddict[key] = h5f[path + \"/\" + key][...]\n",
    "    return ddict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_dat = h5todict(h5file, exclude_names='#refs#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8e+10]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radar_dat['param_records']['radar']['wfs']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/#refs#\" (306 members)>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<HDF5 group \"/custom\" (2 members)>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "<HDF5 group \"/param_qlook\" (29 members)>\n",
      "<HDF5 group \"/param_records\" (27 members)>\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File(oib_2017, 'r')\n",
    "path = \"/\"\n",
    "ddict = {}\n",
    "for key in h5f[path]:\n",
    "    if isinstance(h5f[path + \"/\" + key], h5py._hl.group.Group):\n",
    "        print(h5f[path + \"/\" + key])\n",
    "    else:\n",
    "        ddict[key] = h5f[path + \"/\" + key][...]\n",
    "        print(h5f.get(h5f[path + \"/\" + key].name,getclass=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036, 1)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['Elevation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.group.Group"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5f[path + \"/\" + key].name\n",
    "h5f.get(h5f[path + \"/\" + key].name,getclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5todict(h5file, path=\"/\", exclude_names=None):\n",
    "    with _SafeH5FileRead(h5file) as h5f:\n",
    "        ddict = {}\n",
    "        for key in h5f[path]:\n",
    "            if _name_contains_string_in_list(key, exclude_names):\n",
    "                continue\n",
    "            if is_group(h5f[path + \"/\" + key]):\n",
    "                ddict[key] = h5todict(h5f,\n",
    "                                      path + \"/\" + key,\n",
    "                                      exclude_names=exclude_names)\n",
    "            else:\n",
    "                # Convert HDF5 dataset to numpy array\n",
    "                ddict[key] = h5f[path + \"/\" + key][...]\n",
    "\n",
    "return ddict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wavelet]",
   "language": "python",
   "name": "conda-env-wavelet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
